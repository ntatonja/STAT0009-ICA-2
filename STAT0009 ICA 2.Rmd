---
title: "STAT0009 ICA 2 - Sequential Monte Carlo"
author: '21169367'
date: "11/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Basic Monte Carlo

One of the most common problems in many areas of Statistics is the problem of intractable integrals and the "curse of dimensionality", i.e. that problems become exponentially harder and more computationally intensive when extended to two or more dimensions. The Monte Carlo method helps tackling these issues, and is based on the following principle: if an integral can be written as an expectation with respect to some probability distribution, then we can approximate it by drawing samples from the distribution and computing the sample mean (STAT0017 lecture notes). This method works because of two well known theorems from probability theory, the Law of Large Numbers and the Central Limit Theorem. 

To calculate an expected value $E_p[f(Y)]$, where $Y$ is a random variable with known distribution $p(y)$, and $f(Y)$ is a given function of $Y$, using the Basic Monte Carlo integration, the following algorithm is used. First, for $i = 1,\dots, N$, sample $y^{(i)} ∼ p(y)$, where $y^{(i)}$ denotes the $i^{th}$ sampled value. Then, compute the sample mean to obtain the Monte Carlo estimate $E^{MC}$ of the expected value:$E^{MC}= \frac{1}{N} \sum^{N}_{i=1}f(y^{(i)}).$


To present the most basic Monte Carlo algorithm, we took the following example. Y is a random variable following an ex-Gaussian distribution with parameters $\mu=0.4$, $\sigma=0.1$ and $\tau=0.5$. An ex-Gaussian distribution is often used to model response times, and it is defined as the sum of an exponential and a normally distributed variable, and has three parameters: $\mu$, $\sigma$ and $\tau$, which are the mean and standard deviation of the Gaussian variable, and the rate of the exponential variable, respectively. Y represents the time it takes for a person to complete a given task. We want to estimate the probability that it takes more than 3 seconds for a person to complete this task, $p(Y \geq 3)$. To calculate this, we sampled from an ex-Gaussian distribution and calculated the sample mean of the values that were larger than 3.  Sampling from an ex-Gaussian distribution is easy to do in $\texttt{R}$, using the $\texttt{rexGAUS}$ function in the $\texttt{gamlss.dist}$ package. The following code chunk will demonstrate this.  With a sample size of 2000, this gave an estimate $p(Y ≥ 3) ≈ 0.0045$. This is significantly (around 20%) lower compared to the true value, $p(Y ≥ 3) = 0.0056$.

```{r}
#Tonja's code - not sure if correct
# Sampling - Basic Monte Carlo
set.seed(123)
library(gamlss.dist)
basic <- rexGAUS(2000,0.4,0.1,0.5)
mean(basic>3) #estimate
true <- 1-pexGAUS(3,0.4,0.1,0.5)
1-mean(basic>3)/true #estimated value is around 16% lower than true value

basics <- rep(0,100)
for (i in 1:100) {
  bis <- rexGAUS(2000,0.4,0.1,0.5)
  basics[i] <- mean(bis>3)
}
hist(basics)
```
## Importance sampling

It is clear that the Basic Monte Carlo method does not yield the best estimate in this situation as the exceedance probability we are calculating is very small, hence a larger sample size would be needed to increase precision. Generally speaking, Basic Monte Carlo often fails, as in most cases the distribution we want to sample from, $p$, is unknown, or sampling from it is not always possible or inefficient. An extension of the Basic Monte Carlo method, importance sampling (IS), could help in this situation. Importance sampling allows us to sample from an "instrumental distribution", $q$, instead of $p$, and then the sampled values will be weighted to correct for the fact that they were sampled from $q$, and not the target distribution $p$. This algorithm relies on the algebraic trick of multiplying and dividing by the same quantity, which leads to the "importance sampling fundamental identity" (Robert & Casella, 2004): $E_p[f(Y)] = \int \frac{p(y)}{q(y)}q(y)f(y) dy = E_q[w(Y)f(Y)]$, where $\frac{p(y)}{q(y)}$ is the importance weight. 

Then, the algorithm for importance sampling for an expected value is as follows. First, $i = 1,\dots, N$, sample $y^{(i)} ∼ q(y)$. Then, for $i = 1,\dots, N$, compute the importance weight $w(i) =\frac{p(y^{(i)})}{q(y^{(i)})}$. Lastly, compute a weighted average to obtain the importance sampling estimate:$E^{IS}= \frac{1}{N} \sum^{N}_{i=1}w^{(i)}f(y^{(i)}).$

The choice of the instrumental distribution depends on many things. Ideally, $q$ should be such that sampling from it will be easy, or it will increase the efficiency of the estimate. There is only one restriction on the instrumental distribution, $q$, which is that, if $f(y)p(y) \neq 0$, then $q(y) > 0$, i.e. whenever $p$ assigns non-zero probability to a value $y$, $q$ should do so as well.

It should be noted that importance sampling does not directly generate samples from the target distribution $p$, but it is possible to generate samples which are approximately distributed according to it by resampling with replacement from the
set of sampled values, where we resample a sample value $y^{(i)}$ with a probability proportional to the importance weight $w(y^{(i)})$. This is called the importance sampling resampling algorithm, which will be further explored later on.

Still using the same example, we now try using importance sampling to estimate the exceedance probability $p(Y \geq 3)$, where $Y$ follows an ex-Gaussian distribution with the same parameters as before. We try two different instrumental distributions, first, a normal distribution truncated below at 3, with parameters $\mu = 3$ and $\sigma = 0.1$, then a shifted exponential distribution, shifted to the right by 3, with a rate $\tau = 0.5$. We use a sample size of N = 2000 in both cases. The below code will execute these two importance sampling algorithms. 

```{r}
# Importance Sampling - Truncated Normal
library(truncnorm)
ISnorm <- function(n){
  y <- rep(0,n)
  w <- rep(0,n)
  for(i in 1:n){
    yi <- rtruncnorm(1,3,mean = 3,sd=0.1)
    wi <- max( c(dexGAUS(yi,0.4,0.1,0.5)/dtruncnorm(yi,3,mean = 3,sd=0.1), 0), na.rm = T )
    y[i] <- yi
    w[i] <- wi
  }
  return(list(y = y, w = w))
}
gaussis <- ISnorm(2000)
mean(gaussis$w[gaussis$y>3])
plot(density(gaussis$y*gaussis$w))
plot(density(gaussis$y*gaussis$w), xlim = c(0,0.05))
hist(gaussis$y*gaussis$w, breaks = 120, xlim = c(0,0.2))

gaussiss <- rep(0, 100)
for (i in 1:100) {
  gis <- ISnorm(2000)
  gaussiss[i] <- mean(gis$w[gis$y>3])
}
hist(gaussiss)

# Importance Sampling - Shifted Exponential
ISexp <- function(n){
  y <- rep(0,n)
  w <- rep(0,n)
  for(i in 1:n){
    yi <- rexp(1,0.5)+3
    wi <- dexGAUS(yi,0.4,0.1,0.5)/dexp(yi-3,0.5)
    y[i] <- yi
    w[i] <- wi
  }
  return(list(y = y, w = w))
}
expis <- ISexp(2000)
mean(expis$w)
hist(expis$y*expis$w, breaks = 5, xlim = c(0,0.2))

expiss <- rep(0,100)
for (i in 1:100) {
  eis <- ISexp(2000)
  expiss[i] <- mean(eis$w)
}
hist(expiss)

```
## Efficiency

By looking at the results, it is clear that using the shifted exponential distribution resulted in a much better estimate, which is very close to the true value, whereas the truncated normal one actually yielded a worse result, than basic Monte Carlo. Therefore, it is clear that the choice of the importance distribution strongly affects the outcome. The optimal importance distribution $q*$ would minimise the variance of the estimator, so that $q*(y) =\frac{|f(y)|p(y)}{\int|f(y)|p(y)} dy$. However, the integral in the denominator is often unknown, so this is rarely used in real life. A more practical way to improve the efficiency of the estimators is to normalise the importance weights, so that $W^{(i)} = \frac{w^{(i)}}{\sum_{j=1}^N w^{(j)}}$, which results in the "self-normalized" IS estimator, $E^{ISn}=\sum^N_{i=1}W^{(i)}f(y^{(i)})$. This estimator is biased, however, it diminishes as the sample size increases and is usually offset by the gain in efficiency.


## Sequential Importance Sampling

In real life it is often the case that observations come in sequentially, one after the other. This means that we need to infer any unknown parameters sequentially as well, after each new observation comes in. In a Bayesian context this means that we would need to compute a sequence of posterior distributions. To do this via importance sampling, we would need to define the importance weights as such: $w^{(i)}_t =\frac {p(\theta^{(i)}|y_{1:t})} {q_t(\theta^{(i)})}$, where $q_t(\theta)$ is the importance distribution that generates the importance sample. This method would require high computational power and time, as it would require the generation of a new importance sample at each time point, and this importance sample would get larger and larger over time.

Sequential Importance Sampling (SIS) is an algorithm that could solve this problem as it has an approximately fixed computational cost at each time point. SIS uses information from previous observations and samples, thus it can provide more efficient importance distributions than just using basic importance sampling. Sequential Importance Sampling computes the importance weights incrementally by multiplying the importance weight at the previous time $t-1$ by an incremental weight update $a^{(i)}_t$. The importance weights are defined as follows:$$w^{(i)}_t =\frac {p(\theta^{(i)}|y_{1:t})q_{t-1}(\theta^{(i)})} {p(\theta^{(i)}|y_{1:t-1})q_t(\theta^{(i)})} \frac{p(\theta^{(i)}|y_{1:t-1})}{q_{t-1}(\theta^{(i)})},$$
where the incremental weight update is defined as $$a^{(i)}_t= \frac {p(\theta^{(i)}|y_{1:t})}{p(\theta^{(i)}|y_{1:t-1})}\times \frac{q_{t-1}(\theta^{(i)})}{q_t(\theta^{(i)})}.$$

While this still requires lengthy computations, it can be simplified in certain cases. Assuming that observations are conditionally independent and that the importance distribution is time invariant (i.e. $q_t(\theta) = q_{t-1}(\theta) = q(\theta) $), and using self-normalised importance weights, we can simplify the incremental weight update as $a^{(i)}_t=p(y_t|\theta^{(i)}).$

Then, the algorithm for Sequential Importance Sampling with time invariant parameters is as follows. First, for $i = 1, \dots, N$, sample $\theta^{(i)} ∼ q(\theta )$, then compute the normalised weights $W^{(i)}_0 \propto \frac{p(\theta)}{q(\theta )}$ with $\sum^N_{j=1} W^{(i)}_0 = 1.$ Then, for $t = 1, \dots, t$, reweight the normalised weights for $i = 1, \dots, N$, by computing $W^{(i)}_t \propto p(y_t|\theta^{(i)})W^{(i)}_{t−1}$,with $\sum^N_{i=1} W^{(i)}_t = 1.$ Finally, for $t = 1, \dots, t$ compute the (self-normalised) SIS estimate $E^{SISn}_{t} =\sum ^N_{i=1}W^{(i)}_t f(\theta^{(i)})$.

We will demonstrate how this algorithm works in the following example, whereby we want to sequentially infer the posterior mean and variance of a Gaussian random variable. We assume that the observations are independent and that they come from a Normal distribution with an unknown mean $\mu$ and variance $\sigma^2$. Hence, the unknown parameter $\theta$ is a vector $\theta=(\mu,\sigma)$. Our prior distributions for $\mu$ and $\sigma^2$ - which we will also use as the importance distributions - are a Gaussian distribution with mean 0 and standard deviation 10, and a uniform distribution between 0 and 50, for $\mu$ and $\sigma$ respectively. Then, we apply the algorithm to 100 observations from a Normal(5,5) distribution, using a sample of size N = 200. The following code shows a simulation of this example.

```{r}
SIS <- function(n,times){
  mu <- rep(0,n)
  sig <- rep(0,n)
  obs <- rep(0,times)
  w <- rep(1/n,n)
  wts <- matrix(0,ncol=times,nrow=n)
  for(i in 1:n){
    mu[i] <- rnorm(1,0,10)
    sig[i] <- runif(1,0,50)
  }
  for(i in 1:times){
    obs[i] <- rnorm(1,mean=5,sd=5) 
  }
  for(i in 1:times) {
    w <- w*dnorm(obs[i],mean=mu,sd=sig)
    w <- w/sum(w)
    wts[,i] <- w
  }
  return(list(postmu = colSums(mu*wts), postsig = colSums(sig*wts)))
}
seqposts <- SIS(200,100)
plot(seqposts$postmu, main=expression(mu),xlab="Time point (t)",ylab="Value")
abline(h=5,col="red",lty=2)
plot(seqposts$postsig, main=expression(sigma),xlab="Time point (t)",ylab="Value")
abline(h=5,col="red",lty=2)
```

By looking at the plots above, it is clear that as $t$ increases, the estimated posterior mean of $\sigma$ gets fairly close to the true value, 5, however, the estimated posterior mean of $\mu$ is slightly further away from it. This is due to a problem called weight degeneracy, where the weight of almost all particles becomes negligible as $t$ increases. This results in the posterior mean being essentially estimated by a single sample value, which is not necessarily the one closest to the true value. The reason behind weight degeneracy is that the importance distribution becomes less and less efficient over time. One measure for detecting weight degeneracy is the effective sample size, which is defined as $N^{eff}= \frac {1} {\sum^{N}_{i=1} (W^{(i)})^2}.$ It takes values between 1 and N, so lower effective sample size indicates stronger weight degeneracy.

## Resampling

To overcome the issue of weight degeneracy, an extra step of resampling is introduced in the Sequential Monte Carlo algorithms. Resampling allows for particles to be sampled with replacement from the set of all particles, with a probability that
depends on the importance weights. The key idea is to reproduce particles with large weights and discard those with small weights, as they have little to no effect on the estimates anyway. An additional benefit of resampling is that while the Sequential Importance sampling (SIS) samples were not distributed according to the target distribution $p$ but followed the instrumental distribution $q$, the resampled values are (approximately) distributed according to $p$. 

There are various sampling schemes, such as multinomial resampling, residual resampling, stratified resampling, and systematic resampling. The simplest one is multinomial sampling, which draws $N$ samples from a multinomial distribution for all $i = 1, \dots, N$, with probabilities $p(i) =W^{(i)}$ . After resampling, the weights are set to $W^{(i)} = 1/N$. A drawback of multinomial sampling is that it increases the variance of the estimator. Alternative sampling schemes have been introduced with smaller variances.

Residual resampling is a method that uses a mix of deterministic and random resampling approaches. To keep the estimator unbiased, the expected number of replications of each particle $i$ are set to be equal to $NW^{(i)}$ . As this is not an integer, residual resampling takes the integer part of each $NW^{(i)}$ term and replicates each sample value deterministically according to that number. The remaining particles are then generated through multinomial resampling from a distribution determined by the non-integer parts of each $NW^{(i)}$ term.

Stratified resampling also uses a partially deterministic replication of particles. This method is based on the principles of stratified sampling often used in survey research. It uses the weights to form an "empirical" cumulative distribution over the sampled values, and then this distribution is split into $N$ equally sized strata, and a single draw is taken from each stratum. 

Finally, the most popular resampling scheme, systematic resampling is based on the same idea as stratified resampling, however, it reduces the Monte Carlo variance further by using a single random number instead of different ones, to sample from each stratum. While systematic resampling is simple to implement and usualy performs well, contrary to residual and stratified resampling, it is not guaranteed to outperform multinomial resampling.

The below code shows a simulation for multinomial resampling, using the same example as before for SIS. By comparing the plots from SIS and multinomial resampling, it is clear that the latter performs better than the algorithm without resampling.

```{r}

ReSIS <- function(n,times){
  mu <- rep(0,n)
  sig <- rep(0,n)
  obs <- rep(0,times)
  w <- rep(1/n,n)
  wts <- matrix(0,ncol=times,nrow=n)
  resamplemu <- matrix(0,ncol=n,nrow=times+1)
  resamplesig <- matrix(0,ncol=n,nrow=times+1)
  for(i in 1:n){
    mu[i] <- rnorm(1,0,10)
    sig[i] <- runif(1,0,50)
  }
  resamplemu[1,] <- mu
  resamplesig[1,] <- sig
  for(i in 1:times) {
    obs[i] <- rnorm(1,mean=5,sd=5)
    wt <- w*dnorm(obs[i],mean=resamplemu[i,],sd=resamplesig[i,])
    wt <- wt/sum(wt)
    #wts[,i] <- wt
    resamplemu[i+1,] <- sample(resamplemu[i,], n, replace = T, prob = wt)
    resamplesig[i+1,] <- sample(resamplesig[i,], n, replace = T, prob = wt)
  }
  return(list(postmu = rowMeans(resamplemu), postsig = rowMeans(resamplesig)))
  #return(list(postmu = colSums(mu*wts), postsig = colSums(sig*wts)))
}
seqposts <- ReSIS(20000,100)
plot(seqposts$postmu, main=expression(mu),xlab="Time point (t)",ylab="Value",ylim=c(range(seqposts$postmu)[1],5.2))
abline(h=5,col="red",lty=2)
plot(seqposts$postsig, main=expression(sigma),xlab="Time point (t)",ylab="Value")
abline(h=5,col="red",lty=2)

```

## Particle Filter

```{r}
# Number of discrete time points that 
n <- 50

# Simulate the true latent process theta

# Sample for in initial distribution
intstate <- rnorm(1, 10, sqrt(2))

# Sample the movement of the state
transstates <- rnorm(n-1, 0, 1)

# Initialise the true distribution
theta <- rep(0, n)

# The first value of theta is a sample from the initial distribution
theta[1] <- intstate

# The remaining values of theta are sums of the initial state and the individual transitions
for (i in 2:n) {
  theta[i] <- intstate + cumsum(transstates[1:(i-1)])[i-1]
}

# Add noise to get the observed values of the latent states
y <- theta + rnorm(n, 0, sqrt(10))

# number of particles to use at each distribution
npar <- 500

# The systematic resampling algorithm
reSys <- function(w) {
  # input: w is a vector of length N with (unnormalized) importance weights
  # output: a vector of length N with indices of the replicated particles
  # Find the length of the weights
  N <- length(w)
  # normalise the weights
  w <- w/sum(w)
  # Find the cumulative sum of the weights
  cusum <- cumsum(w)
  # Get the uniform sample
  u <- runif(1, max=1/N)
  # Generate U
  FixedU <- seq(1/N, (N-1)/N, length=N-1)
  U <- c(0, FixedU) + u
  # Initialise the vector of indices
  index <- rep(0,N)
  j <- 1
  # The for loop iterates over j. Each time, if the cumulative sum at j is less than U 
  # at i, then keep increasing j until the cumulative sum is greater. If it is greater,
  # then take that particle when resampling (by taking the index). If the cumulative
  # sum is smaller for a new i, then it will skip over that observation as the weight
  # is too small.
  for(i in 1:N) {
    while (U[i] > cusum[j]) {
      j <- j + 1
    }
    index[i] <- j
  }
  return(index)
}

# Bootstrap filter

# initialise matrix for the particles Each row represents a new time point
# and each column is the number of samples used at each to estimate the true state at
# each time point
P <- matrix(NA, n+1, npar)

# initialise matrix for the weights
W <- matrix(NA, n+1, npar)

# Sample particles from the prior distribution for the initial state.
P[1,] <- rnorm(npar, 10, sqrt(2))
# The particles from the initial distribution are all equally weighted.

# Iterate over each distribution at each time point
for(t in 1:n) {
  # sample particles according to the transition distribution
  P[t+1,] <- rnorm(npar, P[t,], 1)
  # compute the weights for the new distribution according the the previous observed values.
  # The particles at each previous time point have been given uniform weighting, thus
  # there is no need to multiply by hat{W}_{t-1}^{(i)}
  W[t+1,] <- dnorm(y[t], P[t+1,], sqrt(10))
  # Normalise the weights, for each new distribution.
  W[t+1,] <- W[t+1,]/sum(W[t+1,])
  # Particles to be used from systematic resampling.
  P[t+1,] <- P[t+1, reSys(W[t+1,])]
}
plot(1:n, y)
points(theta, col="red")
lines(rowSums(P*W)[-1])
```
