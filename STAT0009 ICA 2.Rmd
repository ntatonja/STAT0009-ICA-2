---
title: "STAT0009 ICA 2 - Sequential Monte Carlo"
author: '21169367, 18003531'
date: "11/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is a report for the published study ‘A tutorial on particle filters’, by Maarten Speekenbrink. It was published in 2016 in the Journal of Mathematical Psychology, and is
accessible here: https://www.sciencedirect.com/science/article/pii/S002224961630030X. Throughout this report we focus on giving a basic understanding to the underlying theory and reproducing some of the illustrated examples through simulation. We only consider chapters 1 through to 4, as chapters beyond this focus more on specific applications and extensions to the methods shown.

## Basic Monte Carlo

One of the most common problems in many areas of Statistics is the problem of intractable integrals and the "curse of dimensionality", i.e. that problems become exponentially harder and more computationally intensive when extended to two or more dimensions. The Monte Carlo method helps tackling these issues, and is based on the following principle: if an integral can be written as an expectation with respect to some probability distribution, then we can approximate it by drawing samples from the distribution and computing the sample mean (STAT0017 lecture notes). This method works because of two well known theorems from probability theory, the Law of Large Numbers and the Central Limit Theorem. 

To calculate an expected value $E_p[f(Y)]$, where $Y$ is a random variable with known distribution $p(y)$, and $f(Y)$ is a given function of $Y$, using the Basic Monte Carlo integration, the following algorithm is used. First, for $i = 1,\dots, N$, sample $y^{(i)} ∼ p(y)$, where $y^{(i)}$ denotes the $i^{th}$ sampled value. Then, compute the sample mean to obtain the Monte Carlo estimate $E^{MC}$ of the expected value:$E^{MC}= \frac{1}{N} \sum^{N}_{i=1}f(y^{(i)}).$


To present the most basic Monte Carlo algorithm, we took the following example. Y is a random variable following an ex-Gaussian distribution with parameters $\mu=0.4$, $\sigma=0.1$ and $\tau=0.5$. An ex-Gaussian distribution is often used to model response times, and it is defined as the sum of an exponential and a normally distributed variable, and has three parameters: $\mu$, $\sigma$ and $\tau$, which are the mean and standard deviation of the Gaussian variable, and the rate of the exponential variable, respectively. Y represents the time it takes for a person to complete a given task. We want to estimate the probability that it takes more than 3 seconds for a person to complete this task, $p(Y \geq 3)$. To calculate this, we sampled from an ex-Gaussian distribution and calculated the sample mean of the values that were larger than 3.  Sampling from an ex-Gaussian distribution is easy to do in $\texttt{R}$, using the $\texttt{rexGAUS}$ function in the $\texttt{gamlss.dist}$ package. The following code chunk will demonstrate this.  With a sample size of 2000, this gave an estimate $p(Y ≥ 3) ≈ 0.0045$. This is significantly (around 20%) lower compared to the true value, $p(Y ≥ 3) = 0.0056$.

Basic Monte Carlo. The following code chunk is structured as follows: first, a necessary library is loaded in to evaluate the ex-Gaussian distribution. The seed is set for reproducability, and samples from the ex-Gaussian distribution are taken. The probability of the samples going above 3 is estimated, and this is compared to the true probability.
```{r}
# Sampling - Basic Monte Carlo

library(gamlss.dist)
set.seed(123)

basic <- rexGAUS(2000, 0.4, 0.1, 0.5)
mean(basic>3)

true <- 1-pexGAUS(3, 0.4, 0.1, 0.5)
1-mean(basic>3)/true

```
## Importance sampling

It is clear that the Basic Monte Carlo method does not yield the best estimate in this situation as the exceedance probability we are calculating is very small, thus a lot of the samples are thrown away when calculating the probability, hence a larger sample size would be needed to increase precision. Generally speaking, Basic Monte Carlo often fails, as in most cases the distribution we want to sample from, $p$, is unknown, or sampling from it is not always possible or inefficient. An extension of the Basic Monte Carlo method, importance sampling (IS), could help in this situation. Importance sampling allows us to sample from an "instrumental distribution", $q$, instead of $p$, and then the sampled values will be weighted to correct for the fact that they were sampled from $q$, and not the target distribution $p$. This algorithm relies on the algebraic trick of multiplying and dividing by the same quantity, which leads to the "importance sampling fundamental identity" (Robert & Casella, 2004): $E_p[f(Y)] = \int \frac{p(y)}{q(y)}q(y)f(y) dy = E_q[w(Y)f(Y)]$, where $\frac{p(y)}{q(y)}$ is the importance weight. 

Then, the algorithm for importance sampling for an expected value is as follows. First, $i = 1,\dots, N$, sample $y^{(i)} ∼ q(y)$. Then, for $i = 1,\dots, N$, compute the importance weight $w(i) =\frac{p(y^{(i)})}{q(y^{(i)})}$. Lastly, compute a weighted average to obtain the importance sampling estimate:$E^{IS}= \frac{1}{N} \sum^{N}_{i=1}w^{(i)}f(y^{(i)}).$

The choice of the instrumental distribution depends on many things. Ideally, $q$ should be such that sampling from it will be easy, or it will increase the efficiency of the estimate. There is only one restriction on the instrumental distribution, $q$, which is that, if $f(y)p(y) \neq 0$, then $q(y) > 0$, i.e. whenever $p$ assigns non-zero probability to a value $y$, $q$ should do so as well.

It should be noted that importance sampling does not directly generate samples from the target distribution $p$, but it is possible to generate samples which are approximately distributed according to it by resampling with replacement from the
set of sampled values, where we resample a sample value $y^{(i)}$ with a probability proportional to the importance weight $w(y^{(i)})$. This is called the importance sampling resampling algorithm, which will be further explored later on.

Still using the same example, we now try using importance sampling to estimate the exceedance probability $p(Y \geq 3)$, where $Y$ follows an ex-Gaussian distribution with the same parameters as before. We try two different instrumental distributions, first, a normal distribution truncated below at 3, with parameters $\mu = 3$ and $\sigma = 0.1$, then a shifted exponential distribution, shifted to the right by 3, with a rate $\tau = 0.5$. We use a sample size of N = 2000 in both cases. The below code will execute these two importance sampling algorithms. 

Importance Sampling. The following code chunk is structured as follows: first, a necessary library is loaded in to evaluate the truncated normal distribution. For the the truncated normal distribution and the shifted exponential distribution, functions are written to generate the importance samples, taking the number of samples wanted as an argument. Importance samples are generated and their weights are calculated, for both distributions. Since probabilities are of interest here, the corresponding function $f$ when calculating the importance sample estimate is just the indicator function, thus only an average of the weights for which the corresponding sample values go above 3 are taken. In the case of the shifted exponential distribution, all sample values are above 3, thus only the average of the weights is calculated.
```{r}
# Load in a necassary library
library(truncnorm)

# Importance Sampling - Truncated Normal
ISnorm <- function(n){
  y <- rep(0,n)
  w <- rep(0,n)
  for(i in 1:n){
    yi <- rtruncnorm(1, 3, mean=3, sd=0.1)
    wi <- max( c(dexGAUS(yi,0.4,0.1,0.5)/dtruncnorm(yi,3,mean=3,sd=0.1), 0), na.rm = T )
    y[i] <- yi
    w[i] <- wi
  }
  return(list(y = y, w = w))
}
gaussis <- ISnorm(2000)
mean(gaussis$w[gaussis$y>3])

# Importance Sampling - Shifted Exponential
ISexp <- function(n){
  y <- rep(0,n)
  w <- rep(0,n)
  for(i in 1:n){
    yi <- rexp(1,0.5)+3
    wi <- dexGAUS(yi,0.4,0.1,0.5)/dexp(yi-3,0.5)
    y[i] <- yi
    w[i] <- wi
  }
  return(list(y = y, w = w))
}
expis <- ISexp(2000)
mean(expis$w)

```

Here, each of the previous 3 methods for sampling are repeated 100 times, each time giving an estimated of the mean. These estimates are then plotted on histograms, so that each of the methods can be compared.
```{r}
# Replicate basic MC samples
basics <- rep(0,100)
for (i in 1:100) {
  bis <- rexGAUS(2000, 0.4, 0.1, 0.5)
  basics[i] <- mean(bis>3)
}

# Replicate Importance samples using a truncated normal
gaussiss <- rep(0, 100)
for (i in 1:100) {
  gis <- ISnorm(2000)
  gaussiss[i] <- mean(gis$w[gis$y>3])
}

# Replicate Importance samples using a shifted exponential
expiss <- rep(0,100)
for (i in 1:100) {
  eis <- ISexp(2000)
  expiss[i] <- mean(eis$w)
}

# Plotting the histograms from all three methods
par(mfrow=c(1,3))
hist(basics,main="Basic Monte Carlo",ylim=c(0,100),xlab="Estimate",xlim=c(0,0.015))
abline(v=0.0056,col="red",lty=2) #this adds a vertical line at the true value

hist(gaussiss, main="Importance sampling (normal)", ylim=c(0,100), xlab="Estimate", xlim=c(0,0.015), breaks=10)
abline(v=0.0056,col="red",lty=2) #this adds a vertical line at the true value

hist(expiss, main="Importance sampling (exponential)", ylim=c(0,100), xlab="Estimate", xlim=c(0,0.015), breaks=2)
abline(v=0.0056,col="red",lty=2) #this adds a vertical line at the true value

par(mfrow=c(1,1))

```


## Efficiency

By looking at the results, it is clear that using the shifted exponential distribution resulted in a much better estimate, which is very close to the true value, whereas the truncated normal one actually yielded a worse result, than basic Monte Carlo. Therefore, it is clear that the choice of the importance distribution strongly affects the outcome. 
Looking at the last plot with the three histograms, we can see that the three estimators have very different variances after we have applied them 100 times. This is due to the large variation in importance weights. Both basic Monte Carlo integration and importance sampling with the truncated Normal provide highly variable estimates, whereas IS with the shifted exponential distribution yields estimates that are tightly clustered around the true value, 0.0056 (indicated by the dashed red line). While these estimates are still unbiased, the large variance of the estimates for Basic Monte Carlo integration and IS with a truncated Normal distribution means that in practice, we are usually far from the true value. 

The optimal importance distribution $q*$ would minimise the variance of the estimator, so that $q*(y) =\frac{|f(y)|p(y)}{\int|f(y)|p(y)} dy$. However, the integral in the denominator is often unknown, so this is rarely used in real life. A more practical way to improve the efficiency of the estimators is to normalise the importance weights, so that $W^{(i)} = \frac{w^{(i)}}{\sum_{j=1}^N w^{(j)}}$, which results in the "self-normalised" IS estimator, $E^{ISn}=\sum^N_{i=1}W^{(i)}f(y^{(i)})$. This estimator is biased, however, it diminishes as the sample size increases and is usually offset by the gain in efficiency.


## Sequential Importance Sampling

In real life it is often the case that observations come in sequentially, one after the other. This means that we need to infer any unknown parameters sequentially as well, after each new observation comes in. In a Bayesian context this means that we would need to compute a sequence of posterior distributions. To do this via importance sampling, we would need to define the importance weights as such: $w^{(i)}_t =\frac {p(\theta^{(i)}|y_{1:t})} {q_t(\theta^{(i)})}$, where $q_t(\theta)$ is the importance distribution that generates the importance sample. This method would require high computational power and time, as it would require the generation of a new importance sample at each time point, and this importance sample would get larger and larger over time.

Sequential Importance Sampling (SIS) is an algorithm that could solve this problem as it has an approximately fixed computational cost at each time point. SIS uses information from previous observations and samples, thus it can provide more efficient importance distributions than just using basic importance sampling. Sequential Importance Sampling computes the importance weights incrementally by multiplying the importance weight at the previous time $t-1$ by an incremental weight update $a^{(i)}_t$. The importance weights are defined as follows:$$w^{(i)}_t =\frac {p(\theta^{(i)}|y_{1:t})q_{t-1}(\theta^{(i)})} {p(\theta^{(i)}|y_{1:t-1})q_t(\theta^{(i)})} \frac{p(\theta^{(i)}|y_{1:t-1})}{q_{t-1}(\theta^{(i)})},$$
where the incremental weight update is defined as $$a^{(i)}_t= \frac {p(\theta^{(i)}|y_{1:t})}{p(\theta^{(i)}|y_{1:t-1})}\times \frac{q_{t-1}(\theta^{(i)})}{q_t(\theta^{(i)})}.$$

While this still requires lengthy computations, it can be simplified in certain cases. Assuming that observations are conditionally independent and that the importance distribution is time invariant (i.e. $q_t(\theta) = q_{t-1}(\theta) = q(\theta)$), and using self-normalised importance weights, we can simplify the incremental weight update as $a^{(i)}_t=p(y_t|\theta^{(i)}).$

Then, the algorithm for Sequential Importance Sampling with time invariant parameters is as follows. First, for $i = 1, \dots, N$, sample $\theta^{(i)} ∼ q(\theta )$, then compute the normalised weights $W^{(i)}_0 \propto \frac{p(\theta)}{q(\theta )}$ with $\sum^N_{j=1} W^{(i)}_0 = 1.$ Then, for $t = 1, \dots, t$, reweight the normalised weights for $i = 1, \dots, N$, by computing $W^{(i)}_t \propto p(y_t|\theta^{(i)})W^{(i)}_{t−1}$,with $\sum^N_{i=1} W^{(i)}_t = 1.$ Finally, for $t = 1, \dots, t$ compute the (self-normalised) SIS estimate $E^{SISn}_{t} =\sum ^N_{i=1}W^{(i)}_t f(\theta^{(i)})$.

We will demonstrate how this algorithm works in the following example, whereby we want to sequentially infer the posterior mean and variance of a Gaussian random variable. We assume that the observations are independent and that they come from a Normal distribution with an unknown mean $\mu$ and variance $\sigma^2$. Hence, the unknown parameter $\theta$ is a vector $\theta=(\mu,\sigma)$. Our prior distributions for $\mu$ and $\sigma$ - which we will also use as the importance distributions - are a Gaussian distribution with mean 0 and standard deviation 10, and a uniform distribution between 0 and 50, for $\mu$ and $\sigma$ respectively. Then, we apply the algorithm to 100 observations from a Normal(5,5) distribution, using a sample of size N = 200. The following code shows a simulation of this example.

Sequential Importance Sampling. The following code chunk is structured as follows: a function is created which takes as inputs the number of samples to use from the prior and the number of time steps in the sequence. Since the prior distributions for $\mu$ and $\sigma$ will also act as instrumental distributions, the initial weights are uniform. The samples from the prior are generated. This is followed by the observations and their corresponding weights being generated at each time point. The weights are normalised, and the posterior expected values for $\mu$ and $\sigma$ are outputted, for each time point. The results from the algorithm being applied are then plotted.
```{r}
# Sequential Importance Sampling
SIS <- function(n, times){
  mu <- rep(0, n)
  sig <- rep(0, n)
  obs <- rep(0, times)
  w <- rep(1/n, n)
  wts <- matrix(0, ncol=times, nrow=n)
  for(i in 1:n){
    mu[i] <- rnorm(1,0,10)
    sig[i] <- runif(1,0,50)
  }
  for(i in 1:times) {
    obs[i] <- rnorm(1,mean=5,sd=5)
    w <- w*dnorm(obs[i],mean=mu,sd=sig)
    w <- w/sum(w)
    wts[,i] <- w
  }
  return(list(postmu = colSums(mu*wts), postsig = colSums(sig*wts)))
}
seqposts <- SIS(200,100)

# Plot the results
plot(seqposts$postmu, main=expression(mu), xlab="Time point (t)", ylab="Value")
abline(h=5,col="red",lty=2)
plot(seqposts$postsig, main=expression(sigma), xlab="Time point (t)", ylab="Value")
abline(h=5,col="red",lty=2)

```

By looking at the plots above, it is clear that as $t$ increases, the estimated posterior mean of $\sigma$ gets fairly close to the true value, 5, however, the estimated posterior mean of $\mu$ is slightly further away from it. This is due to a problem called weight degeneracy, where the weight of almost all particles becomes negligible as $t$ increases. This results in the posterior mean being essentially estimated by a single sample value, which is not necessarily the one closest to the true value. The reason behind weight degeneracy is that the importance distribution becomes less and less efficient over time. One measure for detecting weight degeneracy is the effective sample size, which is defined as $N^{eff}= \frac {1} {\sum^{N}_{i=1} (W^{(i)})^2}.$ It takes values between 1 and N, so lower effective sample size indicates stronger weight degeneracy.

## Resampling

To overcome the issue of weight degeneracy, an extra step of resampling is introduced in the Sequential Monte Carlo algorithms. Resampling allows for particles to be sampled with replacement from the set of all particles, with a probability that
depends on the importance weights. The key idea is to reproduce particles with large weights and discard those with small weights, as they have little to no effect on the estimates anyway. An additional benefit of resampling is that while the Sequential Importance sampling (SIS) samples were not distributed according to the target distribution $p$ but followed the instrumental distribution $q$, the resampled values are (approximately) distributed according to $p$. 

There are various sampling schemes, such as multinomial resampling, residual resampling, stratified resampling, and systematic resampling. The simplest one is multinomial sampling, which draws $N$ samples from a multinomial distribution for all $i = 1, \dots, N$, with probabilities $p(i) =W^{(i)}$ . After resampling, the weights are set to $W^{(i)} = 1/N$. A drawback of multinomial sampling is that it increases the variance of the estimator. Alternative sampling schemes have been introduced with smaller variances.

Residual resampling is a method that uses a mix of deterministic and random resampling approaches. To keep the estimator unbiased, the expected number of replications of each particle $i$ are set to be equal to $NW^{(i)}$ . As this is not an integer, residual resampling takes the integer part of each $NW^{(i)}$ term and replicates each sample value deterministically according to that number. The remaining particles are then generated through multinomial resampling from a distribution determined by the non-integer parts of each $NW^{(i)}$ term.

Stratified resampling also uses a partially deterministic replication of particles. This method is based on the principles of stratified sampling often used in survey research. It uses the weights to form an "empirical" cumulative distribution over the sampled values, and then this distribution is split into $N$ equally sized strata, and a single draw is taken from each stratum. 

Finally, the most popular resampling scheme, systematic resampling is based on the same idea as stratified resampling, however, it reduces the Monte Carlo variance further by using a single random number instead of different ones, to sample from each stratum. While systematic resampling is simple to implement and usualy performs well, contrary to residual and stratified resampling, it is not guaranteed to outperform multinomial resampling.

The below code shows a simulation for multinomial resampling, using the same example as before for SIS. By comparing the plots from SIS and multinomial resampling, it is clear that the latter performs better than the algorithm without resampling.

Sequential Importance Sampling Resampling. The function works almost the same as SIS, excpet at the end there is also a resampling step. As previously stated, after resampling the resampled values are all treated with equal weight, so the average values across the time points is taken and output. The results from the algorithm being applied are then plotted.
```{r}
# Multinomial Resampling
ReSIS <- function(n,times){
  mu <- rep(0, n)
  sig <- rep(0, n)
  obs <- rep(0, times)
  w <- rep(1/n, n)
  wts <- matrix(0, ncol=times, nrow=n)
  resamplemu <- matrix(0, ncol=n, nrow=times+1)
  resamplesig <- matrix(0, ncol=n, nrow=times+1)
  for(i in 1:n){
    mu[i] <- rnorm(1,0,10)
    sig[i] <- runif(1,0,50)
  }
  resamplemu[1,] <- mu
  resamplesig[1,] <- sig
  for(i in 1:times) {
    obs[i] <- rnorm(1, 5, 5)
    wt <- w*dnorm(obs[i], resamplemu[i,], resamplesig[i,])
    wt <- wt/sum(wt)
    resamplemu[i+1,] <- sample(resamplemu[i,], n, replace = T, prob = wt)
    resamplesig[i+1,] <- sample(resamplesig[i,], n, replace = T, prob = wt)
  }
  return(list(postmu = rowMeans(resamplemu), postsig = rowMeans(resamplesig)))
}
seqposts <- ReSIS(20000,100)

# Plot the results
plot(seqposts$postmu, main=expression(mu), xlab="Time point (t)", ylab="Value", ylim=c(range(seqposts$postmu)[1], 5.2))
abline(h=5,col="red",lty=2)
plot(seqposts$postsig, main=expression(sigma), xlab="Time point (t)", ylab="Value")
abline(h=5,col="red",lty=2)

```

## Particle Filters: SMC for state-space models

### State-space models (section 4.1)

When it comes to the filtering problems, we track the hidden states $\phi_{t}$ of a stochastic process as each observation $y_{t}$ comes in. In Bayesian statistics, we do this by computing posterior distributions sequentially, as subsequent dimensions are added - $p\left(\phi_{1} \mid y_{1}\right), \ldots, p\left(\phi_{t} \mid y_{1: t}\right)$. 

To describe time-series of observations state-space models are used. They consist of observations $y_{1: t}$ that are realisations of the hidden states $\phi_{0: t}$. In these type of models, we have two significant assumptions:

- each observation $y_{t}$ depends only on the current state $\phi_{t}$ - we can say that observations are conditionally independent given the hidden states
$$
p\left(y_{1: T} \mid \phi_{0: T}\right)=\prod_{t=1}^{T} p\left(y_{t} \mid \phi_{t}\right)
$$

- the current state depends only on the state at the immediately preceding time point (Markov property):
$$
p\left(\phi_{0: T}\right)=p\left(\phi_{0}\right) \prod_{t=1}^{T} p\left(\phi_{t} \mid \phi_{t-1}\right)
$$

Using these two suppositions, we may write the posterior distribution over the latent states as
$$
p\left(\phi_{0: T} \mid y_{1: T}\right)=\frac{p\left(\phi_{0}\right) \prod_{t=1}^{T} p\left(y_{t} \mid \phi_{t}\right) p\left(\phi_{t} \mid \phi_{t-1}\right)}{p\left(y_{1: T}\right)}
$$

or alternatively - recursive posteriors:

$$
p\left(\phi_{0: t} \mid y_{1: t}\right)=\frac{p\left(y_{t} \mid \phi_{t}\right) p\left(\phi_{t} \mid \phi_{t-1}\right)}{p\left(y_{t} \mid y_{1: t-1}\right)} p\left(\phi_{0: t-1} \mid y_{1: t-1}\right)
$$

where

$$
p\left(y_{t} \mid y_{1: t-1}\right)=\iint p\left(y_{t} \mid \phi_{t}\right) p\left(\phi_{t} \mid \phi_{t-1}\right) p\left(\phi_{t-1} \mid y_{1: t-1}\right) \mathrm{d} \phi_{t-1} \mathrm{~d} \phi_{t} 
$$ 
is a normalising constant.

### SIS for state-space models (section 4.2)

As before, our goal is to estimate some unknown parameter $\theta$, which in the case of state-space models, is a vector of the hidden states $\phi_{0:t}$. Sequential Importance Sampling (SIS) is used, which incrementally builds up the importance sample, starting  with the sample ${\phi^{(i)}_0}$ at t=0, then sampling values $\phi^{(i)}_i$ at times $i=1,..t$ conditional on the previous sample. Then, the importance distribution at time t is defined as $q_t(\phi_{0:t}) = q_t(\phi_t|\phi_{0:t−1})q_{t−1}(\phi_{0:t−1})$. Given that $q_{t−1}(\phi_{0:t}) = q_{t−1}(\phi_{0:t−1})$, and combining this with the importance distribution, the incremental weight update, $a^{(i)}_t$, can be written as $a^{(i)}_t=\frac {p(y_t|\phi^{(i)}_t)p(\phi^{(i)}_t|\phi^{(i)}_{t−1})}{p(y_t|y_{1:t−1})q_t(\phi^{(i)}_t|\phi^{(i)}_{0:t−1})}$. This can be further simplified when using normalised importance weights, as the $p(y_t|y_{1:t−1})$ term can be ignored. As previously with SIS, this algorithm now has approximately constant computational costs, because the weight update works in such a way that there is no need to revisit all the previous observations and hidden states, only the most recent one is conditioned  on. The choice of the importance distribution $q_t(\phi_t|\phi_{0_t-1})$ is still important for minimising the variance of the importance weights. However, even with the "optimal" importance distribution, SIS for state-space models can suffer from the same problem as the basic SIS algorithm; weight degeneracy.

### Example (section 4.4)

In the following example, we will show Sequential Importance Sampling with an example of a latent Gaussian process (observation probability distribution is the normal distribution) with added noise. We define that a hidden variable $\phi$ in discrete time as $\phi_{t+1} = \phi_t + \xi_t$, where $\xi_t \sim N(0, \sigma_{\xi}^{2})$. The initial distribution at $t=0$ of the random walk is stated as $\phi_0 \sim N(\mu_0, \sigma_0^2)$. Moreover, values of observations at time $t=0$ - $Y_t$ depends on the observation probability distribution $\phi_t$ with added noise $\epsilon_t$ such that: $Y_t=\phi_t+\epsilon_t$, $\epsilon_t \sim N(0, \sigma_{\epsilon}^{2} )$. Having this assumptions, we may deduce that the the conditional probability of the observation $Y_t$ given the observation probability distributions $\phi_t$ is normally distributed with mean $\phi_t$ and variance $\sigma_{\epsilon}^{2}$: $p(y_t|\phi_t) = N(\phi_t, \sigma_{\epsilon}^{2})$. Similarly, $p(\phi_t|\phi_{t-1})=N(\phi_{t-1}, \sigma_{\xi}^{2} )$. We assume that observations $y_t$ are made sequentially and after each observation we want to infer corresponding hidden variable $\phi_t$. The distributions that we are interested in are $p(\phi_1|y_1), p(\phi_2|y_{1:2}),...,p(\phi_t|y_{1:t})$. We may compute them analytically using Kalman filter (we are dealing with linear and Gaussian proces), but to demonstrate an approximation of the posterior means
we will use the aforementioned bootstrap filter - one of the versions of a generic particle filter. It is characterized by the fact that the state transition distribution $p(\phi_t|\phi_{t-1})$ is used as a conditional importance distribution $q_t(\phi_t|\phi_{0:t-1})$ - it simplifies the reweighting step and weights are calculated as $W_{t}\propto p\left(y_{t} \mid \phi_{t}^{(i)}\right) \tilde{W}_{t-1}^{(i)}$. In the code below we illustrate this example with $\mu_0=10,\sigma_0^{2}=2, \sigma_{\xi}^{2}=1,\sigma_{\epsilon}^{2}=10$ and number of discrete time points equal to 50. We set $c=1$ - we resample at each repetition using SI. As we can see, we have observations (blue dots), true hidden states calculated with Kalman filter (red dots) and our estimates of the posterior mean in purple.
```{r}
# Bootstrap Filter

# Number of discrete time points that 
n <- 50

# Simulate the true latent process theta

# Sample for in initial distribution
intstate <- rnorm(1, 10, sqrt(2))

# Sample the movement of the state
transstates <- rnorm(n-1, 0, 1)

# Initialise the true distribution
theta <- rep(0, n)

# The first value of theta is a sample from the initial distribution
theta[1] <- intstate

# The remaining values of theta are sums of the initial state and the individual transitions
for (i in 2:n) {
  theta[i] <- intstate + cumsum(transstates[1:(i-1)])[i-1]
}

# Add noise to get the observed values of the latent states
y <- theta + rnorm(n, 0, sqrt(10))

# number of particles to use at each distribution
npar <- 500

# The systematic resampling algorithm
reSys <- function(w) {
  # input: w is a vector of length N with (unnormalized) importance weights
  # output: a vector of length N with indices of the replicated particles
  # Find the length of the weights
  N <- length(w)
  # normalise the weights
  w <- w/sum(w)
  # Find the cumulative sum of the weights
  cusum <- cumsum(w)
  # Get the uniform sample
  u <- runif(1, max=1/N)
  # Generate U
  FixedU <- seq(1/N, (N-1)/N, length=N-1)
  U <- c(0, FixedU) + u
  # Initialise the vector of indices
  index <- rep(0,N)
  j <- 1
  # The for loop iterates over j. Each time, if the cumulative sum at j is less than U 
  # at i, then keep increasing j until the cumulative sum is greater. If it is greater,
  # then take that particle when resampling (by taking the index). If the cumulative
  # sum is smaller for a new i, then it will skip over that observation as the weight
  # is too small.
  for(i in 1:N) {
    while (U[i] > cusum[j]) {
      j <- j + 1
    }
    index[i] <- j
  }
  return(index)
}

# Bootstrap filter

# initialise matrix for the particles Each row represents a new time point
# and each column is the number of samples used at each to estimate the true state at
# each time point
P <- matrix(NA, n+1, npar)

# initialise matrix for the weights
W <- matrix(NA, n+1, npar)

# Sample particles from the prior distribution for the initial state.
P[1,] <- rnorm(npar, 10, sqrt(2))
# The particles from the initial distribution are all equally weighted.

# Iterate over each distribution at each time point
for(t in 1:n) {
  # sample particles according to the transition distribution
  P[t+1,] <- rnorm(npar, P[t,], 1)
  # compute the weights for the new distribution according the the previous observed values.
  # The particles at each previous time point have been given uniform weighting, thus
  # there is no need to multiply by hat{W}_{t-1}^{(i)}
  W[t+1,] <- dnorm(y[t], P[t+1,], sqrt(10))
  # Normalise the weights, for each new distribution.
  W[t+1,] <- W[t+1,]/sum(W[t+1,])
  # Particles to be used from systematic resampling.
  P[t+1,] <- P[t+1, reSys(W[t+1,])]
}
plot(1:n, y, col = "blue", pch = 20, xlab = "Time Points", ylab = "", main = "Gaussian Latent Process")
points(theta, col="red", pch = 20)
lines(0:n, rowSums(P*W), col = "purple")
legend("bottomleft", c("True State", "Observed State", "Particle Filter Estimate"), col = c("red", "blue", "purple"), lty = c(20,20,1))
```
