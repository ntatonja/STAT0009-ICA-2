---
title: "STAT0009 ICA 2 - Sequential Monte Carlo"
author: '21169367'
date: "11/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
One of the most common problems in many areas of Statistics is the problem of intractable integrals and the "curse of dimensionality", i.e. that problems become exponentially harder and more computationally intensive when extended to two or more dimensions. The Monte Carlo method helps tackling these issues, and is based on the following principle: if an integral can be written as an expectation with respect to some probability distribution, then we can approximate it by drawing samples from the distribution and computing the sample mean (STAT0017 lecture notes). This method works because of two well known theorems from probability theory, the Law of Large Numbers and the Central Limit Theorem. 

To calculate an expected value $E_p[f(Y)]$, where $Y$ is a random variable with known distribution $p(y)$, and $f(Y)$ is a given function of $Y$, using the Basic Monte Carlo integration, the following algorithm is used. First, for $i = 1,\dots, N$, sample $y^{(i)} ∼ p(y)$, where $y^{(i)}$ denotes the $i^{th}$ sampled value. Then, compute the sample mean to obtain the Monte Carlo estimate $E^{MC}$ of the expected value:$E^{MC}= \frac{1}{N} \sum^{N}_{i=1}f(y^{(i)}).$


To present the most basic Monte Carlo algorithm, we took the following example. Y is a random variable following an ex-Gaussian distribution with parameters $\mu=0.4$, $\sigma=0.1$ and $\tau=0.5$. An ex-Gaussian distribution is often used to model response times, and it is defined as the sum of an exponential and a normally distributed variable, and has three parameters: $\mu$, $\sigma$ and $\tau$, which are the mean and standard deviation of the Gaussian variable, and the rate of the exponential variable, respectively. Y represents the time it takes for a person to complete a given task. We want to estimate the probability that it takes more than 3 seconds for a person to complete this task, $p(Y \geq 3)$. To calculate this, we sampled from an ex-Gaussian distribution and calculated the sample mean of the values that were larger than 3.  Sampling from an ex-Gaussian distribution is easy to do in $\texttt{R}$, using the $\texttt{rexGAUS}$ function in the $\texttt{gamlss.dist}$ package. The following code chunk will demonstrate this.  With a sample size of 2000, this gave an estimate $p(Y ≥ 3) ≈ 0.0045$. This is significantly (around 20%) lower compared to the true value, $p(Y ≥ 3) = 0.0056$.

```{r}
#Tonja's code - not sure if correct
# Sampling - Basic Monte Carlo
set.seed(123)
library(gamlss.dist)
basic <- rexGAUS(2000,0.4,0.1,0.5)
mean(basic>3) #estimate
true<- 1-pexGAUS(3,0.4,0.1,0.5)
1-mean(basic>3)/true #estimated value is around 16% lower than true value

#------------------------------------------------------------------------------#
#Alex's code 
#exgauss=rnorm(2000,0.4,0.1)+rexp(2000,0.5)
#exgauss[sum(exgauss>3)]/length(exgauss)

exgaussdens <- function(x){
  y <- rep(0,x)
  z <- rep(0,x)
  for (i in 1:x) {
    y[i] <- rexGAUS(1,0.4,0.1,0.5)#rnorm(1,0.4,0.1)+rexp(1,0.5)
    if(y[i]>3) {z[i]=dexGAUS(y[i],0.4,0.1,0.5)
    }
  }
  mean(z[y>3])#sum(z[y>3])/x
}
#plot(density(replicate(10,exgaussdens(2000))))
exgaussdens(2000)

# Very variable
```
It is clear that the Basic Monte Carlo method does not yield the best estimate in this situation as the exceedance probability we are calculating is very small, hence a larger sample size would be needed to increase precision. Generally speaking, Basic Monte Carlo often fails, as in most cases the distribution we want to sample from, $p$, is unknown, or sampling from it is not always possible or inefficient. An extension of the Basic Monte Carlo method, importance sampling (IS), could help in this situation. Importance sampling allows us to sample from an "instrumental distribution", $q$, instead of $p$, and then the sampled values will be weighted to correct for the fact that they were sampled from $q$, and not the target distribution $p$. This algorithm relies on the algebraic trick of multiplying and dividing by the same quantity, which leads to the "importance sampling fundamental identity" (Robert & Casella, 2004): $E_p[f(Y)] = \int \frac{p(y)}{q(y)}q(y)f(y) dy = E_q[w(Y)f(Y)]$. $\frac{p(y)}{q(y)}$ is the importance weight. 

Then, the algorithm for importance sampling for an expected value is as follows. First, $i = 1,\dots, N$, sample $y^{(i)} ∼ q(y)$. Then, for $i = 1,\dots, N$, compute the importance weight $w(i) =\frac{p(y^{(i)})}{q(y^{(i)})}$. Lastly, compute a weighted average to obtain the importance sampling estimate:$E^{IS}= \frac{1}{N} \sum^{N}_{i=1}w^{(i)}f(y^{(i)}).$

The choice of the instrumental distribution depends on many things. Ideally, $q$ should be such that sampling from it will be easy, or it will increase the efficiency of the estimate. There is only one restriction on the instrumental distribution, $q$, which is that, if $f(y)p(y) \neq 0$, then $q(y) > 0$, i.e. whenever $p$ assigns non-zero probability to a value $y$, $q$ should do so as well.

It should be noted that importance sampling does not directly generate samples from the target distribution $p$, but it is possible to generate samples which are approximately distributed according to it by resampling with replacement from the
set of sampled values, where we resample a sample value $y^{(i)}$ with a probability proportional to the importance weight $w(y^{(i)})$. This is called the importance sampling resampling algorithm, which will be further explored later on.

The below codechunk shows 
## Importance sampling

```{r}
library(truncnorm)
# Importance Sampling - Truncated Normal
IS <- function(n){
  y <- rep(0,n)
  w <- rep(0,n)
  for(i in 1:n){
    yi <- rtruncnorm(1,3,mean = 3,sd=0.1)
    wi <- max( c(dexGAUS(yi,0.4,0.1,0.5)/dtruncnorm(yi,3,mean = 3,sd=0.1), 0), na.rm = T )
    y[i] <- yi
    w[i] <- wi
  }
  #y*w
  return(list(y = y, w = w))
}
#plot(density(replicate(10,IS(2000))))
exgaussis <- IS(2000)
mean(exgaussis$w[exgaussis$y>3])

# Importance Sampling - Truncated Exponential
IS <- function(n){
  y <- rep(0,n)
  w <- rep(0,n)
  for(i in 1:n){
    yi <- rexp(1,0.5)+3
    wi <- dexGAUS(yi,0.4,0.1,0.5)/dexp(yi-3,0.5)
    y[i] <- yi
    w[i] <- wi
  }
  #y*w
  return(list(y = y, w = w))
}
#return(list(y = y, w = w))
#plot(density(replicate(10,IS(2000))))
exgaussis <- IS(2000)
mean(exgaussis$w)
#mean((exgaussis$y*exgaussis$w)[exgaussis$y>3])

IS <- function(n){
  y <- rep(0,length(n))
  w <- rep(0,length(n))
  for(i in 1:n){
    y[i] <- rexp(1,0.5)+3
    if(y[i]>3){w[i] <- dexGAUS(y[i],0.4,0.1,0.5)/dexp(y[i]-3,0.5)}
    else{w[i]<-0}
  }
  w <- w/sum(w)
  return(list(y = y, w = w))
}
# This ain't working
exgaussis <- IS(2000)
mean(exgaussis$w)


w <- function(x) dexGAUS(x,0.4,0.1,0.5)/dexp(x-3,0.5)
#f <- function(x) if(x>3){1}else{0}
# For loop would be needed
X <- rexp(2000,0.5)+3
Y <- w(X)#*f(W)
```

## Sequential Importance Sampling

```{r}
SIS <- function(n,times){
  mu <- rep(0,n)
  sig <- rep(0,n)
  obs <- rep(0,times)
  w <- rep(1/n,n)
  wts <- matrix(0,ncol=times,nrow=n)
  for(i in 1:n){
    mu[i] <- rnorm(1,0,10)
    sig[i] <- runif(1,0,50)
  }
  for(i in 1:times){
    obs[i] <- rnorm(1,mean=5,sd=5) 
  }
  for(i in 1:times) {
    w <- w*dnorm(obs[i],mean=mu,sd=sig)
    w <- w/sum(w)
    wts[,i] <- w
  }
  return(list(postmu = colSums(mu*wts), postsig = colSums(sig*wts)))
}
seqposts <- SIS(200,100)
plot(seqposts$postmu)
plot(seqposts$postsig)
```

## Resampling

```{r}

ReSIS <- function(n,times){
  mu <- rep(0,n)
  sig <- rep(0,n)
  obs <- rep(0,times)
  w <- rep(1/n,n)
  wts <- matrix(0,ncol=times,nrow=n)
  resamplemu <- matrix(0,ncol=n,nrow=times+1)
  resamplesig <- matrix(0,ncol=n,nrow=times+1)
  for(i in 1:n){
    mu[i] <- rnorm(1,0,10)
    sig[i] <- runif(1,0,50)
  }
  resamplemu[1,] <- mu
  resamplesig[1,] <- sig
  for(i in 1:times) {
    obs[i] <- rnorm(1,mean=5,sd=5)
    wt <- w*dnorm(obs[i],mean=resamplemu[i,],sd=resamplesig[i,])
    wt <- wt/sum(wt)
    #wts[,i] <- wt
    resamplemu[i+1,] <- sample(resamplemu[i,], n, replace = T, prob = wt)
    resamplesig[i+1,] <- sample(resamplesig[i,], n, replace = T, prob = wt)
  }
  return(list(postmu = rowMeans(resamplemu), postsig = rowMeans(resamplesig)))
  #return(list(postmu = colSums(mu*wts), postsig = colSums(sig*wts)))
}
seqposts <- ReSIS(20000,100)
plot(seqposts$postmu)
plot(seqposts$postsig)

```